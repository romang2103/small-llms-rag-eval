{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TORCH_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary dependencies\n",
    "%pip install --quiet --upgrade \\\n",
    "    langchain langchain-community \\\n",
    "    langchain-faiss jq \\\n",
    "    sentence-transformers transformers \\\n",
    "    evaluate bert-score rouge-score nltk absl-py \\\n",
    "    memory-profiler \\\n",
    "    tiktoken \\\n",
    "    faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import required modules\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing_extensions import List, TypedDict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from bert_score import score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from memory_profiler import memory_usage\n",
    "from transformers import logging\n",
    "import tiktoken\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"credential_area\": \"Artificial intelligence\", \"title\": \"Build Your First Chatbot Using IBM watsonx\\\n",
      "{'source': 'C:\\\\Project\\\\data\\\\course_metadata.json', 'seq_num': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load and parse your course data\n",
    "file_path = './data/course_metadata.json'\n",
    "data = json.loads(Path(file_path).read_text())\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.[]',\n",
    "    text_content=False\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content[:100])\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split course data into 48 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "# Split documents into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "print(f\"Split course data into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding class using gte-Qwen2-1.5B-instruct (CPU-only)\n",
    "class QwenEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str = \"Alibaba-NLP/gte-Qwen2-1.5B-instruct\"):\n",
    "        self.device = \"cpu\"\n",
    "        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self.model.encode(texts, convert_to_numpy=True, device=self.device).tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self.model.encode(text, convert_to_numpy=True, device=self.device).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 12.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings and FAISS vector store\n",
    "embeddings = QwenEmbeddings(model_name=\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\")\n",
    "vector_db = FAISS.from_documents(all_splits, embedding=embeddings)\n",
    "\n",
    "# (Optional) Save FAISS index\n",
    "vector_db.save_local(\"./faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\.venv\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load RAG prompt\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define application state and logic\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    \n",
    "# Retrieve top similar documents from Vector Store\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_db.similarity_search(state[\"question\"] , k=5)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "# Generate response using the LLM and measure time to first token\n",
    "def generate(state: State, model_name: str):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "\n",
    "    start_time = time.time()\n",
    "    buffer = \"\"\n",
    "    ttft = None\n",
    "    total_tokens = 0\n",
    "\n",
    "    for chunk in llm.stream(messages):\n",
    "        if not ttft:\n",
    "            ttft = time.time() - start_time  # time to first token\n",
    "        buffer += chunk.content  # Streamed content pieces\n",
    "        total_tokens += 1\n",
    "    \n",
    "    end_time = time.time() - start_time  # Total time taken\n",
    "    total_tokens = count_tokens(buffer, model_name=model_name)  # or gemma, etc.\n",
    "    # tokens_per_second = total_tokens / end_time if end_time > 0 else 0\n",
    "    tokens_per_second = total_tokens / end_time if end_time > 0 else 0\n",
    "\n",
    "    return {\"answer\": buffer, \"ttft\": ttft, \"tps\": tokens_per_second}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG QA Evaluation\n",
    "\n",
    "# ------------- lexical metrics -------------\n",
    "bleu   = evaluate.load(\"bleu\")\n",
    "rouge  = evaluate.load(\"rouge\")\n",
    "squad  = evaluate.load(\"squad\")          # returns token‑level F1 & EM\n",
    "\n",
    "# ------------- semantic metrics ------------\n",
    "semantic_model = SentenceTransformer(\"all-MiniLM-L6-v2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Import Evaluation Dataset\n",
    "with open(\"./data/eval_dataset.json\", \"r\") as f:\n",
    "\teval_dataset = json.load(f)\n",
    " \n",
    "# eval_dataset = eval_dataset[:3]  # Limit to 3 for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Models to Test\n",
    "models_to_test = {\n",
    "    # Small-sized LLMs\n",
    "    # \"phi4-mini:latest\": \"phi4-mini:latest\",\n",
    "    # \"gemma3:1b\": \"gemma3:1b\",\n",
    "    # \"gemma3:4b\": \"gemma3:4b\",\n",
    "    # \"llama3.2:1b\": \"llama3.2:1b\",\n",
    "    # \"llama3.2:latest\": \"llama3.2:latest\",\n",
    "    \n",
    "    # Quantised models\n",
    "    \"BF16\": \"hf.co/unsloth/gemma-3-4b-it-GGUF:BF16\",\n",
    "    \"Q4_0\": \"hf.co/unsloth/gemma-3-4b-it-GGUF:Q4_0\",\n",
    "    \"Q8_0\": \"hf.co/unsloth/gemma-3-4b-it-GGUF:Q8_0\",\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ✅ Metric Functions\n",
    "def compute_bert_score(pred, ref):\n",
    "    P, R, F1 = score([pred], [ref], lang=\"en\", verbose=False)\n",
    "    return F1[0].item()\n",
    "\n",
    "def compute_bleu(pred, ref):\n",
    "    return bleu.compute(predictions=[pred], references=[ref])[\"bleu\"]\n",
    "\n",
    "def compute_rouge(pred, ref):\n",
    "    return rouge.compute(predictions=[pred], references=[ref])[\"rougeL\"]\n",
    "\n",
    "def compute_semantic_sim(pred, ref):\n",
    "    pred_emb = semantic_model.encode([pred])\n",
    "    ref_emb = semantic_model.encode([ref])\n",
    "    return cosine_similarity(pred_emb, ref_emb)[0][0]\n",
    "\n",
    "def compute_faithfulness(answer: str, context_docs: List[Document]) -> float:\n",
    "    context_text = \" \".join(doc.page_content for doc in context_docs)\n",
    "    context_embedding = semantic_model.encode([context_text], convert_to_numpy=True)\n",
    "    answer_embedding = semantic_model.encode([answer], convert_to_numpy=True)\n",
    "    return cosine_similarity(answer_embedding, context_embedding)[0][0]\n",
    "\n",
    "def compute_recall_at_5(context_docs: List[Document], ref: str) -> float:\n",
    "    match_found = any(ref.strip().lower() in doc.page_content.lower() for doc in context_docs[:5])\n",
    "    return 1.0 if match_found else 0.0\n",
    "\n",
    "def count_tokens(text: str, model_name: str):\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model_name)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rag_pipeline(q: str , model_name: str) -> dict:\n",
    "    state: State = {\"question\": q, \"context\": [], \"answer\": \"\"}\n",
    "    state.update(retrieve(state))\n",
    "    state.update(generate(state, model_name))\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "NUM_RUNS = 3\n",
    "RESULTS_DIR = \"./results/quantised\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "run_dfs = []\n",
    "\n",
    "for i in range(1, NUM_RUNS + 1):\n",
    "    print(f\"\\n🔁 Starting RAG evaluation run {i}/{NUM_RUNS}...\\n\")\n",
    "    \n",
    "    # Run the main evaluation logic\n",
    "    results = []\n",
    "\n",
    "    for model_name, model_id in models_to_test.items():\n",
    "        print(f\"Evaluating model: {model_name}\")\n",
    "        counter = 0\n",
    "        llm = init_chat_model(model_id, model_provider=\"ollama\", stream=True)\n",
    "\n",
    "        for item in eval_dataset:\n",
    "            counter += 1\n",
    "            q, ref, type = item[\"question\"], item[\"answer\"], item[\"type\"]\n",
    "            try:\n",
    "                start = time.time()\n",
    "                mem_usage, state = memory_usage((run_rag_pipeline, (q, model_name), {}), retval=True, interval=0.01)\n",
    "                ttft = state.get(\"ttft\", -1)\n",
    "                tps = state.get(\"tps\", -1)\n",
    "                total_time = time.time() - start\n",
    "                peak_memory_mb = max(mem_usage)\n",
    "\n",
    "                answer = state[\"answer\"]\n",
    "                retrieved_docs = state[\"context\"]\n",
    "\n",
    "                # Metrics\n",
    "                bleu_score = compute_bleu(answer, ref)\n",
    "                rouge_score = compute_rouge(answer, ref)\n",
    "                prediction = {\"id\": str(counter), \"prediction_text\": answer}\n",
    "                reference = {\"id\": str(counter), \"answers\": {\"text\": [ref], \"answer_start\": [0]}}\n",
    "                qa_scores = squad.compute(predictions=[prediction], references=[reference])\n",
    "                f1_score = qa_scores[\"f1\"]\n",
    "                bert_score_val = compute_bert_score(answer, ref)\n",
    "                semantic_sim = compute_semantic_sim(answer, ref)\n",
    "                faithfulness_score = compute_faithfulness(answer, retrieved_docs)\n",
    "                recall_score = compute_recall_at_5(retrieved_docs, ref)\n",
    "\n",
    "                results.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Question\": q,\n",
    "                    \"Question Type\": type,\n",
    "                    \"Expected Answer\": ref,\n",
    "                    \"Generated Answer\": answer,\n",
    "                    \"BLEU\": bleu_score,\n",
    "                    \"ROUGE-L\": rouge_score,\n",
    "                    \"F1\": f1_score,\n",
    "                    \"BERTScore (F1)\": bert_score_val,\n",
    "                    \"Semantic Sim\": semantic_sim,\n",
    "                    \"Recall@5\": recall_score,\n",
    "                    \"Faithfulness\": faithfulness_score,\n",
    "                    \"Answer Length\": len(answer.split()),\n",
    "                    \"Time to First Token (s)\": round(ttft, 3),\n",
    "                    \"Tokens per Second\": round(tps, 3),\n",
    "                    \"Total Time (s)\": total_time,\n",
    "                    \"Peak Memory (MB)\": round(peak_memory_mb, 2)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error on item {counter}: {e}\")\n",
    "                results.append({\n",
    "                    \"Model\": model_name,\n",
    "                    \"Question\": q,\n",
    "                    \"Question Type\": type,\n",
    "                    \"Expected Answer\": ref,\n",
    "                    \"Generated Answer\": str(e),\n",
    "                    \"BLEU\": 0, \"ROUGE-L\": 0, \"F1\": 0, \"BERTScore (F1)\": 0,\n",
    "                    \"Semantic Sim\": 0, \"Recall@5\": 0, \"Faithfulness\": 0,\n",
    "                    \"Answer Length\": 0, \"Time to First Token (s)\": -1,\n",
    "                    \"Tokens per Second\": -1, \"Total Time (s)\": -1, \"Peak Memory (MB)\": 0\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    run_path = os.path.join(RESULTS_DIR, f\"rag_evaluation_results_{i}.csv\")\n",
    "    df.to_csv(run_path, index=False)\n",
    "    run_dfs.append(df)\n",
    "    print(f\"✅ Run {i} saved to {run_path}\")\n",
    "\n",
    "# ✅ Average metrics across runs\n",
    "merged_df = pd.concat(run_dfs)\n",
    "avg_df = merged_df.groupby([\"Model\", \"Question\", \"Question Type\", \"Expected Answer\"], as_index=False).mean(numeric_only=True)\n",
    "avg_df.to_csv(os.path.join(RESULTS_DIR, \"rag_evaluation_results_avg.csv\"), index=False)\n",
    "print(f\"\\n✅ Averaged results saved to {RESULTS_DIR}/rag_evaluation_results_avg.csv\")\n",
    "\n",
    "# ✅ Block-by-type summary like rag_model_blocks_2.csv\n",
    "metrics = [\n",
    "    \"BLEU\", \"ROUGE-L\", \"F1\", \"BERTScore (F1)\", \"Semantic Sim\", \"Recall@5\", \"Faithfulness\",\n",
    "    \"Answer Length\", \"Time to First Token (s)\", \"Tokens per Second\", \"Total Time (s)\", \"Peak Memory (MB)\"\n",
    "]\n",
    "\n",
    "blocks = []\n",
    "for qtype, df_group in avg_df.groupby(\"Question Type\"):\n",
    "    block = df_group.groupby(\"Model\")[metrics].mean()\n",
    "    block.insert(0, \"Question Type\", qtype)\n",
    "    blocks.append(block)\n",
    "    blocks.append(pd.DataFrame([[\"\"] * block.shape[1]], columns=block.columns))  # spacer\n",
    "\n",
    "final_block_df = pd.concat(blocks, axis=0)\n",
    "final_block_path = os.path.join(RESULTS_DIR, \"rag_model_blocks_avg.csv\")\n",
    "final_block_df.to_csv(final_block_path, index=True)\n",
    "print(f\"📊 Block-wise summary saved to {final_block_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
